{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/pickle/preproc/df_patient_admit_icu__20210119.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "stopWords.extend(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n",
    "stopWords = set(stopWords)\n",
    "\n",
    "def remove_stopwords(diagnosis_list):\n",
    "    return [d for d in diagnosis_list if d not in stopWords]\n",
    "\n",
    "def cleanup_raw_diagnoses(series):\n",
    "    series = series.str.strip()\n",
    "    \n",
    "    cleaned = []\n",
    "    regex_split = re.compile(r'[\\|/;|,]')\n",
    "    regex_sub1 = re.compile(r\"[\\|/\\.-]+\")\n",
    "    for ix,diagnosis in enumerate(series):\n",
    "        if pd.isna(diagnosis):\n",
    "            diagnosis = ''\n",
    "        \n",
    "        diagnosis = diagnosis.replace('\\\\',' ')\n",
    "        diagnosis = diagnosis.replace(\"'\",' ')\n",
    "        diagnosis_list = regex_split.split(diagnosis)\n",
    "        diagnosis_list = [d.strip() for d in diagnosis_list]\n",
    "        diagnosis_list = remove_stopwords(diagnosis_list)\n",
    "        \n",
    "        diagnosis = ' '.join(diagnosis_list)\n",
    "        diagnosis = regex_sub1.sub(' ', diagnosis)\n",
    "        cleaned.append(diagnosis)\n",
    "        \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleanup_raw_diagnoses(df['DIAGNOSIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def diagnosis_tokenizer(diagnosis):\n",
    "    return word_tokenize(diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []\n",
    "for diagn in cleaned:\n",
    "    tokens_list.extend(diagnosis_tokenizer(diagn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count', CountVectorizer(max_features=3000)),\n",
       "                ('tfid', TfidfTransformer(sublinear_tf=True))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('count', CountVectorizer(max_features=3000)),\n",
    "                 ('tfid', TfidfTransformer(sublinear_tf=True))])\n",
    "\n",
    "pipe.fit(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_names=pipe.named_steps['count'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.69512168, 11.98280375, 11.98280375, ..., 12.38826886,\n",
       "       11.98280375, 11.0019745 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['tfid'].idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vals = pipe['tfid'].idf_\n",
    "sorted_tfids = sorted(zip(tfidf_names,tfidf_vals),key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features=3000)\n",
    "vec_fit = vec.fit_transform(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_names = vec.get_feature_names();    \n",
    "count_vals = np.asarray(vec_fit.sum(axis=0))[0]\n",
    "sorted_counts = sorted(zip(count_names,count_vals),key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import ColumnDataSource, figure, output_notebook,show\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.models import CustomJS, Slider\n",
    "import numpy as np\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=7\n",
    "ix1 = 0\n",
    "ix2 = 100\n",
    "\n",
    "foo = [w[0] for w in sorted_counts[ix1:ix2]]\n",
    "bar = [c[1] for c in sorted_counts[ix1:ix2]]\n",
    "\n",
    "p1 = figure(x_range=foo,background_fill_color=\"#fafafa\")\n",
    "p1.vbar(x=foo, top=bar, width=0.9)\n",
    "p1.xaxis.major_label_orientation = 'vertical'\n",
    "\n",
    "baz = [w[0] for w in sorted_tfids[ix1:ix2]]\n",
    "bam = [c[1] for c in sorted_tfids[ix1:ix2]]\n",
    "\n",
    "p2 = figure(x_range=baz,background_fill_color=\"#fafafa\")\n",
    "p2.vbar(x=baz, top=bam, width=0.9)\n",
    "p2.xaxis.major_label_orientation = 'vertical'\n",
    "\n",
    "show(row(p1,p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(foo)\n",
    "print(baz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from customTransformers import ColumnSelectTransformer, DiagnosisFrameTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "count_pipe = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['DIAGNOSIS'])),\n",
    "    ('dst', DiagnosisFrameTransformer()),\n",
    "    ('count',CountVectorizer(max_features=3000)),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "tfidf_pipe = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['DIAGNOSIS'])),\n",
    "    ('dst', DiagnosisFrameTransformer()),\n",
    "    ('count',CountVectorizer(max_features=3000)),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'rfc__criterion': ['gini','entropy'],\n",
    "    'rfc__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'rfc__min_samples_split': [2,5,8,11],\n",
    "    'rfc__min_samples_leaf': [1,4,7],\n",
    "    'rfc__class_weight': ['balanced', 'balanced_subsample'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 16.5min finished\n"
     ]
    }
   ],
   "source": [
    "gs_regressor = GridSearchCV(count_pipe, params, cv=KFold(n_splits=5, shuffle=True),verbose=3,n_jobs=-1)\n",
    "count_est = gs_regressor.fit(df,df['SAMEDAY_ADM_TO_ICU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'rfc__criterion': ['entropy'],\n",
    "    'rfc__max_features': ['auto'],\n",
    "    'rfc__min_samples_split': [2],\n",
    "    'rfc__min_samples_leaf': [1],\n",
    "    'rfc__class_weight': ['balanced_subsample'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   43.2s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   43.7s finished\n"
     ]
    }
   ],
   "source": [
    "gs_regressor = GridSearchCV(tfidf_pipe, params, cv=KFold(n_splits=5, shuffle=True),verbose=3,n_jobs=-1)\n",
    "tfidf_est = gs_regressor.fit(df,df['SAMEDAY_ADM_TO_ICU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rfc__class_weight': 'balanced_subsample',\n",
       "  'rfc__criterion': 'entropy',\n",
       "  'rfc__max_features': 'auto',\n",
       "  'rfc__min_samples_leaf': 1,\n",
       "  'rfc__min_samples_split': 2},\n",
       " 0.730375594554699)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_est.best_params_,count_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rfc__class_weight': 'balanced_subsample',\n",
       "  'rfc__criterion': 'entropy',\n",
       "  'rfc__max_features': 'auto',\n",
       "  'rfc__min_samples_leaf': 1,\n",
       "  'rfc__min_samples_split': 2},\n",
       " 0.7362965392816139)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_est.best_params_,tfidf_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tfidf_est.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10847,  3264],\n",
       "       [ 8951, 37908]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df['SAMEDAY_ADM_TO_ICU'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
