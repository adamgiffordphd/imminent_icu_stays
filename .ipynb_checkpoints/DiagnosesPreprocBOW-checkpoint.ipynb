{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/pickle/preproc/df_patient_admit_icu__20210119.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "stopWords.extend(['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'])\n",
    "stopWords = set(stopWords)\n",
    "\n",
    "def remove_stopwords(diagnosis_list):\n",
    "    return [d for d in diagnosis_list if d not in stopWords]\n",
    "\n",
    "def cleanup_raw_diagnoses(series):\n",
    "    series = series.str.strip()\n",
    "    \n",
    "    cleaned = []\n",
    "    regex_split = re.compile(r'[\\|/;|,]')\n",
    "    regex_sub1 = re.compile(r\"[\\|/\\.-]+\")\n",
    "    for ix,diagnosis in enumerate(series):\n",
    "        if pd.isna(diagnosis):\n",
    "            diagnosis = ''\n",
    "        \n",
    "        diagnosis = diagnosis.replace('\\\\',' ')\n",
    "        diagnosis = diagnosis.replace(\"'\",' ')\n",
    "        diagnosis_list = regex_split.split(diagnosis)\n",
    "        diagnosis_list = [d.strip() for d in diagnosis_list]\n",
    "        diagnosis_list = remove_stopwords(diagnosis_list)\n",
    "        \n",
    "        diagnosis = ' '.join(diagnosis_list)\n",
    "        diagnosis = regex_sub1.sub(' ', diagnosis)\n",
    "        cleaned.append(diagnosis)\n",
    "        \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleanup_raw_diagnoses(df['DIAGNOSIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def diagnosis_tokenizer(diagnosis):\n",
    "    return word_tokenize(diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = []\n",
    "for diagn in cleaned:\n",
    "    tokens_list.extend(diagnosis_tokenizer(diagn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count', CountVectorizer(max_features=3000)),\n",
       "                ('tfid', TfidfTransformer(sublinear_tf=True))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('count', CountVectorizer(max_features=3000)),\n",
    "                 ('tfid', TfidfTransformer(sublinear_tf=True))])\n",
    "\n",
    "pipe.fit(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_names=pipe.named_steps['count'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.69512168, 11.98280375, 11.98280375, ..., 12.38826886,\n",
       "       11.98280375, 11.0019745 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['tfid'].idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vals = pipe['tfid'].idf_\n",
    "sorted_tfids = sorted(zip(tfidf_names,tfidf_vals),key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features=3000)\n",
    "vec_fit = vec.fit_transform(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_names = vec.get_feature_names();    \n",
    "count_vals = np.asarray(vec_fit.sum(axis=0))[0]\n",
    "sorted_counts = sorted(zip(count_names,count_vals),key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import ColumnDataSource, figure, output_notebook,show\n",
    "from bokeh.layouts import row, column\n",
    "from bokeh.models import CustomJS, Slider\n",
    "import numpy as np\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=7\n",
    "ix1 = 0\n",
    "ix2 = 100\n",
    "\n",
    "foo = [w[0] for w in sorted_counts[ix1:ix2]]\n",
    "bar = [c[1] for c in sorted_counts[ix1:ix2]]\n",
    "\n",
    "p1 = figure(x_range=foo,background_fill_color=\"#fafafa\")\n",
    "p1.vbar(x=foo, top=bar, width=0.9)\n",
    "p1.xaxis.major_label_orientation = 'vertical'\n",
    "\n",
    "baz = [w[0] for w in sorted_tfids[ix1:ix2]]\n",
    "bam = [c[1] for c in sorted_tfids[ix1:ix2]]\n",
    "\n",
    "p2 = figure(x_range=baz,background_fill_color=\"#fafafa\")\n",
    "p2.vbar(x=baz, top=bam, width=0.9)\n",
    "p2.xaxis.major_label_orientation = 'vertical'\n",
    "\n",
    "show(row(p1,p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(foo)\n",
    "print(baz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from customTransformers import ColumnSelectTransformer, DiagnosisFrameTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "count_pipe = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['DIAGNOSIS'])),\n",
    "    ('dst', DiagnosisFrameTransformer()),\n",
    "    ('count',CountVectorizer(max_features=3000)),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "tfidf_pipe = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['DIAGNOSIS'])),\n",
    "    ('dst', DiagnosisFrameTransformer()),\n",
    "    ('count',CountVectorizer(max_features=3000)),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'rfc__criterion': ['gini','entropy'],\n",
    "    'rfc__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'rfc__min_samples_split': [2,5,8,11],\n",
    "    'rfc__min_samples_leaf': [1,4,7],\n",
    "    'rfc__class_weight': ['balanced', 'balanced_subsample'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  96 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 256 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 480 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed: 16.5min finished\n"
     ]
    }
   ],
   "source": [
    "gs_regressor = GridSearchCV(count_pipe, params, cv=KFold(n_splits=5, shuffle=True),verbose=3,n_jobs=-1)\n",
    "count_est = gs_regressor.fit(df,df['SAMEDAY_ADM_TO_ICU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'rfc__criterion': ['entropy'],\n",
    "    'rfc__max_features': ['auto'],\n",
    "    'rfc__min_samples_split': [2],\n",
    "    'rfc__min_samples_leaf': [1],\n",
    "    'rfc__class_weight': ['balanced_subsample'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   43.2s remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   43.7s finished\n"
     ]
    }
   ],
   "source": [
    "gs_regressor = GridSearchCV(tfidf_pipe, params, cv=KFold(n_splits=5, shuffle=True),verbose=3,n_jobs=-1)\n",
    "tfidf_est = gs_regressor.fit(df,df['SAMEDAY_ADM_TO_ICU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rfc__class_weight': 'balanced_subsample',\n",
       "  'rfc__criterion': 'entropy',\n",
       "  'rfc__max_features': 'auto',\n",
       "  'rfc__min_samples_leaf': 1,\n",
       "  'rfc__min_samples_split': 2},\n",
       " 0.730375594554699)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_est.best_params_,count_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rfc__class_weight': 'balanced_subsample',\n",
       "  'rfc__criterion': 'entropy',\n",
       "  'rfc__max_features': 'auto',\n",
       "  'rfc__min_samples_leaf': 1,\n",
       "  'rfc__min_samples_split': 2},\n",
       " 0.7362965392816139)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_est.best_params_,tfidf_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = tfidf_est.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10847,  3264],\n",
       "       [ 8951, 37908]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(df['SAMEDAY_ADM_TO_ICU'],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = pd.read_pickle('./data/pickle/preproc/df_patient_admit_icu__20210126_TRAIN_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from customTransformers import ColumnSelectTransformer, DiagnosisFrameTransformer, EstimatorTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   31.8s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  2.5min\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Done 240 out of 240 | elapsed:  7.0min finished\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "## TRYING TO IMPLEMENT 4 OVR CLASSIFICATION SYSTEMS FOR ICU_URGENCY.\n",
    "# 1. CREATE AND TUNE OVR for demographics\n",
    "# 2. use multinomial classifier for diagnoses\n",
    "# 3. use all 5 outputs as features in a second multinomial to pool results\n",
    "\n",
    "cols = ['GENDER','ADMISSION_TYPE','ADMISSION_LOCATION','INSURANCE','LANGUAGE',\n",
    "       'RELIGION','MARITAL_STATUS','ETHNICITY','ADMIT_AGE']\n",
    "\n",
    "demog_feats = FeatureUnion([\n",
    "    ('stdscl', ColumnTransformer([('numerical', StandardScaler(), ['ADMIT_AGE'])])),\n",
    "    ('ohe',  ColumnTransformer([('categorical', OneHotEncoder(handle_unknown='ignore'), cols)]))\n",
    "])\n",
    "\n",
    "demog_pipe = Pipeline([\n",
    "    ('features', demog_feats),\n",
    "    ('reg', LogisticRegression(solver='saga'))\n",
    "])\n",
    "demog_params = {\n",
    "    'reg__penalty': ['l1','l2','none'],\n",
    "    'reg__C': [0.01, 0.1, 1, 10],\n",
    "    'reg__class_weight': ['none','balanced'],\n",
    "    'reg__multi_class': ['ovr','multinomial'],\n",
    "}\n",
    "\n",
    "lin_gs_regressor = GridSearchCV(demog_pipe, demog_params, cv=StratifiedKFold(n_splits=5, shuffle=True),verbose=3,n_jobs=4)\n",
    "lin_est = lin_gs_regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'reg__C': 0.1,\n",
       "  'reg__class_weight': 'none',\n",
       "  'reg__multi_class': 'ovr',\n",
       "  'reg__penalty': 'l2'},\n",
       " 0.6278292394917798)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_est.best_params_,lin_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed: 27.3min\n",
      "[Parallel(n_jobs=4)]: Done 504 tasks      | elapsed: 50.2min\n",
      "[Parallel(n_jobs=4)]: Done 720 out of 720 | elapsed: 69.7min finished\n"
     ]
    }
   ],
   "source": [
    "diagn_pipe = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['DIAGNOSIS'])),\n",
    "    ('dst', DiagnosisFrameTransformer()),\n",
    "    ('count',CountVectorizer(max_features=3000)),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    ('rfc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "diagn_params = {\n",
    "    'rfc__criterion': ['gini','entropy'],\n",
    "    'rfc__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'rfc__min_samples_split': [2,5,8,11],\n",
    "    'rfc__min_samples_leaf': [1,4,7],\n",
    "    'rfc__class_weight': ['balanced', 'balanced_subsample'],\n",
    "}\n",
    "\n",
    "diagn_gs_regressor = GridSearchCV(diagn_pipe, diagn_params, cv=StratifiedKFold(n_splits=5, shuffle=True),verbose=3,n_jobs=4)\n",
    "tfidf_est = diagn_gs_regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rfc__class_weight': 'balanced',\n",
       "  'rfc__criterion': 'gini',\n",
       "  'rfc__max_features': 'sqrt',\n",
       "  'rfc__min_samples_leaf': 1,\n",
       "  'rfc__min_samples_split': 2},\n",
       " 0.58750192130317)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_est.best_params_,tfidf_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed: 34.4min\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed: 58.0min\n",
      "[Parallel(n_jobs=4)]: Done 324 out of 324 | elapsed: 64.0min finished\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n",
      "/Users/adamgifford/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'xgb__criterion': 'friedman_mse',\n",
       "  'xgb__learning_rate': 0.05,\n",
       "  'xgb__loss': 'deviance',\n",
       "  'xgb__max_depth': 3,\n",
       "  'xgb__max_features': 'auto',\n",
       "  'xgb__min_samples_leaf': 7,\n",
       "  'xgb__subsample': 0.8},\n",
       " 0.6116328951484848)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "demog_est = LogisticRegression(solver='saga', C=0.1, class_weight='none', \n",
    "                               penalty='l2', multi_class='ovr')\n",
    "demog_pipe = Pipeline([\n",
    "    ('features', demog_feats),\n",
    "    ('reg', demog_est)\n",
    "])\n",
    "\n",
    "diagn_est = RandomForestClassifier(class_weight='balanced',criterion='gini',\n",
    "                                  max_features='sqrt',min_samples_leaf=1,\n",
    "                                  min_samples_split=2)\n",
    "diagn_pipe = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(['DIAGNOSIS'])),\n",
    "    ('dst', DiagnosisFrameTransformer()),\n",
    "    ('count',CountVectorizer(max_features=3000)),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "    ('rfc', diagn_est)\n",
    "])\n",
    "\n",
    "union = FeatureUnion([\n",
    "    ('demog', EstimatorTransformer(demog_pipe)),# FeatureUnions use the same syntax as Pipelines\n",
    "    ('diagn',EstimatorTransformer(diagn_pipe))\n",
    "])\n",
    "\n",
    "\n",
    "full_pipe = Pipeline([\n",
    "    ('union',union),\n",
    "    ('xgb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "xgb_params = {\n",
    "    'xgb__loss': ['deviance','exponential'],\n",
    "    'xgb__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'xgb__subsample': [0.8, 1], # max is 1\n",
    "    'xgb__criterion': ['friedman_mse'],\n",
    "    'xgb__min_samples_leaf': [1, 4, 7],\n",
    "    'xgb__max_depth': [2,3,4],\n",
    "    'xgb__max_features': ['auto']\n",
    "}\n",
    "\n",
    "xgb_gs_regressor = GridSearchCV(full_pipe, xgb_params, cv=StratifiedKFold(n_splits=3, shuffle=True),verbose=3,n_jobs=4)\n",
    "xgb_est = xgb_gs_regressor.fit(X_train,LabelEncoder().fit(y_train).transform(y_train))\n",
    "\n",
    "xgb_est.best_params_,xgb_est.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = pd.read_pickle('./data/pickle/preproc/df_patient_admit_icu__20210126_TEST_final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5963  528   19  224]\n",
      " [ 828  553   71  221]\n",
      " [ 731  243   72  103]\n",
      " [1255  350   63  970]]\n",
      "0.6198130227980975\n",
      "0.5807983189727202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "y_test_ = LabelEncoder().fit(y_test).transform(y_test)\n",
    "y_pred = xgb_est.predict(X_test)\n",
    "print(confusion_matrix(y_test_,y_pred))\n",
    "print(accuracy_score(y_test_,y_pred))\n",
    "print(f1_score(y_test_,y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48776"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = './data/pickle/models/log_rf_xgb__URGENCY__20210127.pkl'\n",
    "pickle.dump((demog_pipe, diagn_pipe, xgb_est),open(file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
